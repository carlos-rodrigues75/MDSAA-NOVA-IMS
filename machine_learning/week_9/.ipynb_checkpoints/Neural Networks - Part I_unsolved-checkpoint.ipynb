{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f46d35",
   "metadata": {},
   "source": [
    "# Neural Networks - Part I\n",
    "\n",
    "[<font color='#E8800A'>1 - Implement a neural network: methods and evaluation</font>](#one-bullet) <br>\n",
    "[<font color='#E8800A'>2 - The attributes of a Neural Network</font>](#two-bullet) <br>\n",
    "[<font color='#E8800A'>3 - Some parameters in Neural Networks</font>](#three-bullet) <br>\n",
    "    [<font color='#E8800A'>3.1. - The hidden layer</font>](#four-bullet) <br>\n",
    "    [<font color='#E8800A'>3.2. - The maximum iterations</font>](#five-bullet) <br>\n",
    "\n",
    "[<font color='#E8800A'>4 - Comparing LR, DT and NN </font>](#six-bullet) <br>        \n",
    "[<font color='#E8800A'>5 - The importance of scaling (EXERCISE)</font>](#seven-bullet) <br>\n",
    "    [<font color='#E8800A'>5.1. - MinMax Scaler</font>](#eight-bullet) <br>\n",
    "    [<font color='#E8800A'>5.2. - Standard Scaler</font>](#nine-bullet) <br>\n",
    "    [<font color='#E8800A'>5.3. - Robust Scaler</font>](#ten-bullet) <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "#install mlxtend\n",
    "#!pip install mlxtend\n",
    "\n",
    "#import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590a7ee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a class=\"anchor\" id=\"one-bullet\">     \n",
    "\n",
    "# 1. Implement a neural network: methods and evaluation\n",
    "</a>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f660a",
   "metadata": {},
   "source": [
    "__`Step 1`__ Load the dataset Diabetes, define the independent variables and the target and apply train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(r'diabetes.csv')\n",
    "X = diabetes.iloc[:,:-1]\n",
    "y = diabetes.iloc[:,-1]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, random_state = 15, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10cc670",
   "metadata": {},
   "source": [
    "__`Step 2`__ Import MLPClassifier from sklearn.neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eaaa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964ecb1",
   "metadata": {},
   "source": [
    "__`Step 3`__ Using MLPClassifier, create a MLP Classifier instance called model and fit to your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506697b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d9953",
   "metadata": {},
   "source": [
    "__`Step 4`__ Check the f1 score and the mean accuracy in validation for the model you created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pred_val = model.predict(X_val)\n",
    "print('F1 Score:' ,f1_score(y_val, pred_val))\n",
    "print('Mean Accuracy:' ,model.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3014c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a class=\"anchor\" id=\"two-bullet\">     \n",
    "\n",
    "# 2 - The attributes of a Neural Network\n",
    "</a>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff511b7",
   "metadata": {},
   "source": [
    "__`Step 5`__ Using MLPClassifier, create a MLP Classifier instance called model2, and define the parameter __hidden_layer_sizes = (3)__, and define the __verbose = 10__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74cfa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLPClassifier(hidden_layer_sizes = 2, verbose = True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ab07a",
   "metadata": {},
   "source": [
    "The error is calculated by quantifying the difference between the predicted output and the desired output. This difference is called \"loss\" and the function used to calculate the difference is called the \"loss function\". In this case, the model is optimized by using the log-loss function / cross-entropy:\n",
    "\n",
    "$$log\\_loss = -\\frac{1}{N}\\sum_{N}^{i=1}y_{i}\\cdot log(p(y_{i}))\\cdot log(1-p(y_{i}))$$\n",
    "\n",
    "where $y$ is the label and $p(y)$ is the predicted probability of the point being 1 for all N points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc461601",
   "metadata": {},
   "source": [
    "__`Step 6`__ Plot the loss through the iterations by calling the attribute __loss_curve___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "losses = model2.loss_curve_\n",
    "iterations = range(model2.n_iter_)\n",
    "sns.lineplot(x = iterations, y = losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd0e1dc",
   "metadata": {},
   "source": [
    "__`Step 7`__ Obtain the final loss of your model by calling the attribute __loss___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ede1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19698fe5",
   "metadata": {},
   "source": [
    "__`Step 8`__ Get the weight matrix by calling the attribute __coefs___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ff7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.coefs_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09cdb8a",
   "metadata": {},
   "source": [
    "<img src=\"image1_nn.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3947f4",
   "metadata": {},
   "source": [
    "__`Step 9`__ Get the weights between the input and the hidden layer by calling the attribute __coefs_[0]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.coefs_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8815850",
   "metadata": {},
   "source": [
    "__`Step 10`__ Get the weights between the hidden layer and the output by calling the attribute __coefs_[1]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.coefs_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851fa539",
   "metadata": {},
   "source": [
    "__`Step 11`__ Get the bias vector by calling the attribute __intercepts___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.intercepts_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156f5a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a class=\"anchor\" id=\"three-bullet\">     \n",
    "\n",
    "# 3 - Some parameters in Neural Networks\n",
    "</a>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0468e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score(model):\n",
    "    # apply kfold\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    # create lists to store the results from the different models \n",
    "    score_train = []\n",
    "    score_val = []\n",
    "    timer = []\n",
    "    n_iter = []\n",
    "    \n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        # get the indexes of the observations assigned for each partition\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        # start counting time\n",
    "        begin = time.perf_counter()\n",
    "        # fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "        # finish counting time\n",
    "        end = time.perf_counter()\n",
    "        # check the mean accuracy for the train\n",
    "        value_train = model.score(X_train, y_train)\n",
    "        # check the mean accuracy for the validation\n",
    "        value_val = model.score(X_val,y_val)\n",
    "        # append the accuracies, the time and the number of iterations in the corresponding list\n",
    "        score_train.append(value_train)\n",
    "        score_val.append(value_val)\n",
    "        timer.append(end-begin)\n",
    "        n_iter.append(model.n_iter_)\n",
    "    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n",
    "    avg_time = round(np.mean(timer),3)\n",
    "    avg_train = round(np.mean(score_train),3)\n",
    "    avg_val = round(np.mean(score_val),3)\n",
    "    std_time = round(np.std(timer),2)\n",
    "    std_train = round(np.std(score_train),2)\n",
    "    std_val = round(np.std(score_val),2)\n",
    "    avg_iter = round(np.mean(n_iter),1)\n",
    "    std_iter = round(np.std(n_iter),1)\n",
    "    \n",
    "    return str(avg_time) + '+/-' + str(std_time), str(avg_train) + '+/-' + str(std_train),\\\n",
    "str(avg_val) + '+/-' + str(std_val), str(avg_iter) + '+/-' + str(std_iter)\n",
    "\n",
    "def show_results(df, *args):\n",
    "    \"\"\"\n",
    "    Receive an empty dataframe and the different models and call the function avg_score\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    # for each model passed as argument\n",
    "    for arg in args:\n",
    "        # obtain the results provided by avg_score\n",
    "        time, avg_train, avg_val, avg_iter = avg_score(arg)\n",
    "        # store the results in the right row\n",
    "        df.iloc[count] = time, avg_train, avg_val, avg_iter\n",
    "        count+=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90334c6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<a class=\"anchor\" id=\"four-bullet\">    \n",
    "\n",
    "### 3.1. - The hidden layer\n",
    " </a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20892d55",
   "metadata": {},
   "source": [
    "__The number of hidden layers__<br>\n",
    "-\tIncrease the number of hidden layers might improve the accuracy or might not, it depend on the complexity of the problem\n",
    "-\tIncrease the number of hidden layers more than the sufficient ones will cause overfit on training set and the decrease of the accuracy in the validation set\n",
    "\n",
    "__The number of hidden units__ <br>\n",
    "-\tUsing too few neurons in hidden layers will result in underfitting\n",
    "-\tUsing too many neurons in the hidden layer may result in overfitting and increase the time it takes to train the neural network\n",
    "\n",
    "The aim is to keep a good trade-off between the simplicity of the model and the performance accuracy! <br>\n",
    "\n",
    "__Some rule of thumbs:__\n",
    "-\tThe number of hidden neurons should be between the size of the input layer and the size of the output layer\n",
    "-\tThe number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer\n",
    "-\tThe number of hidden neurons should be less than twice the size of the input layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2632d6",
   "metadata": {},
   "source": [
    "__`Step 12`__ Create an MLPClassifier with one hidden layer and one neuron and name it __model_simple__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = MLPClassifier(hidden_layer_sizes=(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66e05a",
   "metadata": {},
   "source": [
    "__`Step 13`__ Create an MLPClassifier with one hidden layer and 8 neurons and name it __model_medium__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8c09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_medium = MLPClassifier(hidden_layer_sizes=(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b1b067",
   "metadata": {},
   "source": [
    "__`Step 14`__ Create an MLPClassifier with four hidden layers and 100 neurons each and name it __model_complex__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_complex = MLPClassifier(hidden_layer_sizes=(100,100,100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59247dd4",
   "metadata": {},
   "source": [
    "__`Step 15`__ Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04168021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Validation', 'Iterations'], index = ['Simple','Medium','Complex'])\n",
    "show_results(df, model_simple, model_medium, model_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98492ab8",
   "metadata": {},
   "source": [
    "While the results may differ in different runs, we probably will get the following conclusions:\n",
    "- The more complex the model, the higher the running time;\n",
    "- We can have a boost on the performance on our model when we adjust rightly the complexity of it - too simple leads to underfitting, and too complex can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30bcef0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<a class=\"anchor\" id=\"five-bullet\">    \n",
    "\n",
    "### 3.2. - The maximum iterations\n",
    " </a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4cccb",
   "metadata": {},
   "source": [
    "By default, sklearn defines the maximum number of iterations as 200. While this could be enough for simple datasets, in complex problems you should try values higher that allow the model to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b462652",
   "metadata": {},
   "source": [
    "__`Step 16`__ Create an instance of MLPClassifier, define the max_iter as __20__, the __hidden_layer_sizes=(8)__ and name it as __model_maxiter_20__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00552c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_maxiter_20 = MLPClassifier(max_iter = 20, hidden_layer_sizes=(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cf65e",
   "metadata": {},
   "source": [
    "__`Step 17`__ Create an instance of MLPClassifier, define the max_iter as __200__, the __hidden_layer_sizes=(8)__ and name it as __model_maxiter_200__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_maxiter_200 = MLPClassifier(max_iter = 200, hidden_layer_sizes=(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167807f",
   "metadata": {},
   "source": [
    "__`Step 18`__ Create an instance of MLPClassifier, define the max_iter as __1000__, the __hidden_layer_sizes=(8)__ and name it as __model_maxiter_1000__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_maxiter_1000 = MLPClassifier(max_iter = 1000, hidden_layer_sizes=(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739a72c",
   "metadata": {},
   "source": [
    "__`Step 19`__ Check the mean accuracy of each model by calling the function _show_results_ and pass as arguments the dataset and the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebf234",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['Time','Train','Validation', 'Iterations'], index = ['max iter 20','max iter 200','max iter 1000'])\n",
    "show_results(df, model_maxiter_20, model_maxiter_200, model_maxiter_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d27840b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a class=\"anchor\" id=\"six-bullet\">     \n",
    "\n",
    "# 4 - Comparing LR, DT and NN\n",
    "</a>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832edd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlxtend\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# 1) generate data for make_classification problem\n",
    "X, y = make_classification(n_features=2, n_redundant=0, random_state=5, n_informative=2, n_clusters_per_class=1)\n",
    "\n",
    "# 2) artificially introduce 5 outliers\n",
    "n_out = 5\n",
    "X_out = np.vstack((np.random.uniform(0.5, 1.5, n_out), np.random.uniform(-3, -2.5, n_out))).transpose()\n",
    "y_out = np.zeros(X_out.shape[0])\n",
    "\n",
    "# 3) split the data into training and unseen (validation)\n",
    "X_training, X_unseen, y_training, y_unseen = train_test_split(X, y, train_size=0.7, random_state=5)\n",
    "# create an object named as X_train_out that will contain the ouliers\n",
    "X_train_out = np.vstack((X_training, X_out))\n",
    "# create an object named y_train_out that will contain the observations with outliers\n",
    "y_train_out = np.concatenate((y_training, y_out)).astype(int)\n",
    "\n",
    "# 4) estimate models without outliers \n",
    "# 4) 1) instantiate and train a Logistic Regression model\n",
    "lr = LogisticRegression(random_state=5)\n",
    "lr.fit(X_training, y_training)\n",
    "# 4) 2) instantiate and train a Decision Tree model\n",
    "dt = DecisionTreeClassifier(random_state=5)\n",
    "dt.fit(X_training, y_training)\n",
    "# 4) 3) instantiate and train a MLP model\n",
    "mlp = MLPClassifier(random_state=5, max_iter = 1000)\n",
    "mlp.fit(X_training, y_training)\n",
    "\n",
    "# 5) estimate models with outliers \n",
    "# 5) 1) instantiate and train a Logistic Regression model\n",
    "lr_out = LogisticRegression(random_state=5)\n",
    "lr_out.fit(X_train_out, y_train_out)\n",
    "# 5) 2) instantiate and train a Decision Tree model\n",
    "dt_out = DecisionTreeClassifier(random_state=5)\n",
    "dt_out.fit(X_train_out, y_train_out)\n",
    "# 5) 3) instantiate and train a MLP model\n",
    "mlp_out = MLPClassifier(random_state=5, max_iter = 1000)\n",
    "mlp_out.fit(X_train_out, y_train_out)\n",
    "\n",
    "# 6) plot classification boundaries\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# 6) 1) LR without outliers\n",
    "plt.subplot(231)\n",
    "plot_decision_regions(X=X, y=y, clf=lr)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"LR's validation accuracy: {0:.2f}%\".format(100*(lr.score(X_unseen, y_unseen))))\n",
    "\n",
    "# 6) 2) DT without outliers\n",
    "plt.subplot(232)\n",
    "plot_decision_regions(X=X, y=y, clf=dt)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"DT's validation accuracy: {0:.2f}%\".format(100*(dt.score(X_unseen, y_unseen))))\n",
    "\n",
    "# 6) 3) MLP without outliers\n",
    "plt.subplot(233)\n",
    "plot_decision_regions(X=X, y=y, clf=mlp)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"MLP's validation accuracy: {0:.2f}%\".format(100*(mlp.score(X_unseen, y_unseen))))\n",
    "\n",
    "# 6) plot classification boundaries\n",
    "X = np.vstack((X, X_out))\n",
    "y = np.concatenate((y, y_out)).astype(int)\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# 6) 4) LR with outliers\n",
    "plt.subplot(234)\n",
    "plot_decision_regions(X=X, y=y, clf=lr_out, X_highlight=X_out,\n",
    "                          scatter_highlight_kwargs = {'s': 120, 'label': 'Outliers', 'alpha': 0.7})\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"LR's validation accuracy (with outliers): {0:.2f}%\".format(100*(lr_out.score(X_unseen, y_unseen))))\n",
    "\n",
    "# 6) 5) DT with outliers\n",
    "plt.subplot(235)\n",
    "plot_decision_regions(X=X, y=y, clf=dt_out, X_highlight=X_out,\n",
    "                          scatter_highlight_kwargs = {'s': 120, 'label': 'Outliers', 'alpha': 0.7})\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"DT's validation accuracy (with outliers): {0:.2f}%\".format(100*(dt_out.score(X_unseen, y_unseen))))\n",
    "\n",
    "# 6) 5) MLP with outliers\n",
    "plt.subplot(236)\n",
    "plot_decision_regions(X=X, y=y, clf=mlp_out, X_highlight=X_out,\n",
    "                          scatter_highlight_kwargs = {'s': 120, 'label': 'Outliers', 'alpha': 0.7})\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"MLP's validation accuracy (with outliers): {0:.2f}%\".format(100*(mlp_out.score(X_unseen, y_unseen))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb8cd5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a class=\"anchor\" id=\"seven-bullet\">     \n",
    "\n",
    "# 5 - The importance of scaling (EXERCISE)\n",
    "</a>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762c1a6",
   "metadata": {},
   "source": [
    "In most cases, your dataset will contain features that differ significantly in terms of magnitudes, units and range. Due to the fact that the majority of machine learning algorithms use Euclidean distance between two data points in their computations, this can be a problem.<br><br>\n",
    "\n",
    "If left on the original format, those algorithms will consider significantly the magnitude of features and this could lead to biased results. <br><br>\n",
    "As an example, if we consider a variable as weight, the algorithm will work differently if that feature is measured in kilograms or in grams - having a value of 5000 grams will give more \"weight\" to the feature than having a value of 5 kilograms. <br>The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To suppress this effect, we need to bring all features to the same level of magnitudes.\n",
    "\n",
    "It is a good practice to normalize the data by putting its mean to zero and its variance to one, or to rescale it by fixing the minimum and the maximum between -1 and 1 or 0 and +1. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176407a1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<a class=\"anchor\" id=\"eight-bullet\">    \n",
    "\n",
    "### 5.1. - MinMax Scaler\n",
    " </a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a677f94",
   "metadata": {},
   "source": [
    "__`Step 20`__ Import __MinMaxScaler__ from __sklearn.preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11891db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994492da",
   "metadata": {},
   "source": [
    "MinMaxScaler is the simplest method and consists in rescaling the range of features to scale in range in [0,1] or [-1,1]. The general formula is:\n",
    "\n",
    "$$x^{'} = \\frac{x - min(x)}{max(x) - min(x)}$$\n",
    "\n",
    "where $x$ is the original value and $x^{'}$ is the normalized one. <br>\n",
    "To rescale between a tuple of values [a,b], the formula becomes: <br>\n",
    "\n",
    "$$x^{'} = a + \\frac{(x - min(x))(b-a)}{max(x) - min(x)}$$\n",
    "\n",
    "where $a$ is the minimum value and $b$ the maximum value.\n",
    "\n",
    "`When to use` \n",
    "- Use this as the first scaler choice to transform a feature, as it will preserve the shape of the dataset (no distortion);\n",
    "- Least disruptive to the information in the original data.\n",
    "\n",
    "`Note`\n",
    "- Doesn't reduce the importance of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ad654",
   "metadata": {},
   "source": [
    "#### 5.1.1. Rescale between [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d7174",
   "metadata": {},
   "source": [
    "__`Step 21`__ Create an object named __min_max1__ that will contain an instance of __MinMaxScaler()__ fitted to your __X_train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ac901d",
   "metadata": {},
   "source": [
    "__`Step 22`__ Create an object named __min_max1_X_train__ that will contain the __X_train__ scaled, by calling the method __transform()__ to your fitted instance created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c5ed0",
   "metadata": {},
   "source": [
    "__`Step 23`__ Similarly to Step 22, create an object named as __min_max1_X_val__ that will transform your __X_val__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becac9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5d63f",
   "metadata": {},
   "source": [
    "__`Step 24`__ Create an object named __model_min_max1__ that will contain a MLPClassifier model fitted to your scaled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ed3ba",
   "metadata": {},
   "source": [
    "__`Step 25`__ Calculate the mean accuracy of your classifier in your __min_max1_X_val__ and __y_val__ by calling the method __score()__ and define it as __score_minmax1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341c93f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcba68f",
   "metadata": {},
   "source": [
    "#### 2.1.2. Rescale between [-1,1]\n",
    "\n",
    "__`Step 26`__ Similarly to the steps included in the _**topic 2.1.2.**_ , this time apply MinMaxScaler(), but define the parameter __feature_range = (-1,1)__. Check the mean accuracy of your new classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bce69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler instance that will range between -1 and 1 and fit to your train data\n",
    "min_max2 = #CODE HERE\n",
    "\n",
    "# Transform your train data by applying the scale obtained in the previous command\n",
    "min_max2_X_train = #CODE HERE\n",
    "\n",
    "# Transform your validation data by applying the scale obtained in the first command\n",
    "min_max2_X_val = #CODE HERE\n",
    "\n",
    "# Create a new classifier and fit to your scaled training data\n",
    "model_min_max2 = #CODE HERE\n",
    "\n",
    "# Check the mean accuracy of your classifier in your validation data\n",
    "model_min_max2.score(min_max2_X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb67197",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<a class=\"anchor\" id=\"nine-bullet\">    \n",
    "\n",
    "### 5.1. - Standard Scaler\n",
    " </a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af633a74",
   "metadata": {},
   "source": [
    "__`Step 27`__ Import __StandardScaler__ from __sklearn.preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93969caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6477956",
   "metadata": {},
   "source": [
    "StandardScaler applies standardization to data. <br>\n",
    "It standardizes a feature by removing the mean and dividing each value by the standard deviation. The formula is:\n",
    "\n",
    "\n",
    "$$x^{'} = \\frac{x - \\bar{x}}{\\sigma }$$\n",
    "\n",
    "where $x^{'}$ is the standardized value, $x$ is the original feature, $\\bar{x}$ is the average of $x$ and $\\sigma$ is its standard deviation.\n",
    "\n",
    "`When to use` \n",
    "- Use it if you know the data distribution is normal;\n",
    "\n",
    "`Note`\n",
    "- If you have outliers in your variable, applying StandardScaler will scale most of the data to a small interval;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b59fd",
   "metadata": {},
   "source": [
    "__`Step 28`__ Create an object named __scaler__ that will contain an instance of __StandardScaler()__ fitted to your __X_train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c49e3",
   "metadata": {},
   "source": [
    "__`Step 29`__ Transform your training dataset using the scaler and define it as a new object named __scaler_X_train__. Do the same for your validation data , and name the new object as __scaler_X_val__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55212520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985e9d2",
   "metadata": {},
   "source": [
    "__`Step 30`__ Create a MLPClassifier named __model_scaler__ and fit to your scaled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9326045c",
   "metadata": {},
   "source": [
    "__`Step 31`__ Check the mean accuracy of your previous model in your validation data by calling the method __score()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31629dc8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<a class=\"anchor\" id=\"ten-bullet\">    \n",
    "\n",
    "### 5.3. - Robust Scaler\n",
    " </a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b4c8d",
   "metadata": {},
   "source": [
    "__`Step 32`__ Import __RobustScaler__ from __sklearn.preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560652e",
   "metadata": {},
   "source": [
    "The RobustScaler scales the data according to the quantile range. <br>\n",
    "It is an alternative for StandardScaler when in presence of outliers, because those influence the mean/variance in a negative way. <br>In those cases, the interquartile range often give better results. The formula is:\n",
    "\n",
    "\n",
    "$$x^{'} = \\frac{x - Q_{1}(x)}{Q_{3}(x) - Q_{1}(x)}$$\n",
    "\n",
    "where $x^{'}$ is the scaled value, $x$ is the original feature, $Q_{1}(x)$ is the first quantile of the feature and  $Q_{3}(x)$ is the third quantile.\n",
    "\n",
    "`When to use` \n",
    "- Use if you have outliers and don't want them to have much influence (but removing outliers is always a better option);\n",
    "- Outliers have less influence than in other methods. The range is larger than MinMaxScaler or StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7d0d8",
   "metadata": {},
   "source": [
    "__`Step 33`__ Similarly to what was done in the previous scalers, apply this time the RobustScaler and check the mean accuracy of a model fitted to your new scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e8319",
   "metadata": {},
   "source": [
    "__`Step 34`__ Plot the different results using a barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3d5df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = ['raw','MinMax[0-1]','MinMax[-1,1]','Standard','Robust']\n",
    "y = [model.score(X_val, y_val), model_min_max1.score(min_max1_X_val, y_val), \n",
    "    model_min_max2.score(min_max2_X_val, y_val),model_scaler.score(scaler_X_val, y_val),\n",
    "    model_robust.score(robust_X_val, y_val)]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x = x, y = y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
