{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font size=\"6\">Naive Bayes</font><a class=\"anchor\"><a id='toc'></a></b><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to apply Naive Bayes in Python.\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png\" width=\"400px\"/> <br><br>\n",
    "\n",
    "On a very fundamental level Bayes' theorem, or Bayes rule, is a mathematical formula used to calculate the conditional probability of events in statistics and probability theory. Its mathematical proof is presented below:\n",
    "\n",
    "We start with the definition of joint probability:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A \\vert B) P(B)\n",
    "$$\n",
    "\n",
    "This can be rearranged to give the definition of conditional probability:\n",
    "\n",
    "$$\n",
    "P(A \\vert B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Similarly, we can express $P(B \\vert A)$ as:\n",
    "\n",
    "$$\n",
    "P(B \\vert A) = \\frac{P(A \\cap B)}{P(A)}\n",
    "$$\n",
    "\n",
    "Taking the ratio of these two expressions gives us:\n",
    "\n",
    "$$\n",
    "\\frac{P(A \\vert B)}{P(B \\vert A)}= \\frac{P(B)}{P(A)}\n",
    "$$\n",
    "\n",
    "Rearranging this equation leads us to Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(A \\vert B)= \\frac{P(B \\vert A) P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Another way of looking at it uses slightly different terminology. We can think of Bayes’ theorem as a way to update knowledge when presented with new evidence or data:</b>\n",
    "1. We hold a preconceived notion or belief as **prior**\n",
    "2. We calculate the likelihood of the data conditioned on our **prior**\n",
    "3. The prior uses the data to be updated into a **posterior**\n",
    "\n",
    "In more formal terms, given hypothesis ***y*** and evidence **X**:\n",
    "\n",
    "$$\n",
    "P(y \\vert X) = \\frac{P(X \\vert y) P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "* P( X | *y* ) - the **likelihood of the evidence if the hypothesis was correct**\n",
    "* P(*y*) – the **prior**, what we believed before we saw the evidence\n",
    "* P(X) - the **likelihood of that evidence under any circumstances**\n",
    "* P( *y* | X ) - the **posterior** (i.e. the updated prior)\n",
    "\n",
    "Bayesian classification uses Bayes' theorem to predict class membership probabilities (i.e. the probability that a given sample belongs to a specific class). \n",
    "\n",
    "To classify **y** given $X$, we would need to be able to compute different probabilities:\n",
    "1. The prior $P(y)$,\n",
    "2. The probability distribution of the data $P(X)$,\n",
    "3. The likelihood of the data conditioned on the prior $P( X \\vert y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "# TOC<a class=\"anchor\"><a id='toc'></a></b><br>\n",
    "- [<font color='#E8800A'>Understanding Naive Bayes using Categorical Data</font>](#first-bullet)<br>\n",
    "* [<font color='#E8800A'>Gaussian Naive Bayes</font>](#second-bullet) <br>\n",
    "- [<font color='#E8800A'>Exercise</font>](#third-bullet)<br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Naive Bayes for Categorical Data</font><a class=\"anchor\" id=\"second-bullet\"></a> \n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"company\">\n",
    "\n",
    "## Weather Data\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I present you a small dataset with only categorical attributes about playing or not tennis and we will build a Naive Bayes classifier from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1:`__ Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather.csv')\n",
    "\n",
    "#windy is a boolean value, so we need to convert it to a string to use it for this exercise\n",
    "weather['WINDY'] = weather['WINDY'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OUTLOOK</th>\n",
       "      <th>TEMPERATURE</th>\n",
       "      <th>HUMIDITY</th>\n",
       "      <th>WINDY</th>\n",
       "      <th>PLAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>True</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>True</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>True</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>True</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     OUTLOOK TEMPERATURE HUMIDITY  WINDY PLAY\n",
       "0      Sunny         Hot     High  False   No\n",
       "1      Sunny         Hot     High   True   No\n",
       "2   Overcast         Hot     High  False  Yes\n",
       "3      Rainy        Mild     High  False  Yes\n",
       "4      Rainy        Cool   Normal  False  Yes\n",
       "5      Rainy        Cool   Normal   True   No\n",
       "6   Overcast        Cool   Normal   True  Yes\n",
       "7      Sunny        Mild     High  False   No\n",
       "8      Sunny        Cool   Normal  False  Yes\n",
       "9   Overcast        Mild   Normal  False  Yes\n",
       "10     Sunny        Mild   Normal   True  Yes\n",
       "11  Overcast        Mild     High   True  Yes\n",
       "12  Overcast         Hot   Normal  False  Yes\n",
       "13     Rainy        Mild     High   True   No"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with regards to our dataset, we can apply Bayes’ theorem in following way:\n",
    "\n",
    "$$P(Play|features)=\\frac{P(feature|Play)*P(Play)}{P(features)}$$\n",
    "\n",
    "Training the model, in Bayesian logic, means to calculate the probabilities on the right side of the equation. Making a prediction means to calculate the probability on the left side of the equation for each class and select the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume this instance: `today = (Outlook = Sunny, Temperature = Cool, Humidity = High, Windy = True)`<br>\n",
    "\n",
    "To make a prediction, we need to calculate the probability of **Play** being **Yes** or **No** given the independent features. Variable **Play** has two possible outcomes: **Yes** and **No**.<br>\n",
    "\n",
    "$$P(Yes|today)=\\frac{P(today|Yes)*P(Yes)}{P(today)}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P(No|today)=\\frac{P(today|No)*P(No)}{P(today)}$$\n",
    "\n",
    "Computing these values in the real world is difficult:<br>\n",
    "1. There most likely are dependencies between features\n",
    "2. The relative importance of each feature is not obvious\n",
    "3. We do not know, and cannot know, the *real* probability distribution of the data\n",
    "\n",
    "How difficult you ask? Well, let's see the disentanglement of P(today|Play):\n",
    "\n",
    "$P(Sunny, Cool, High, True|Yes) = w_1P(Sunny|Cool, High, True, Yes) * w_2P(Cool|Sunny, High, True, Yes) * w_3P(High|Sunny, Cool, True, Yes) * w_4P(True|Sunny, Cool, High, Yes)$\n",
    "\n",
    "and\n",
    "\n",
    "$P(Sunny, Cool, High, True|No) = w_1P(Sunny|Cool, High, True, No) * w_2P(Cool|Sunny, High, True, No) * w_3P(High|Sunny, Cool, True, No) * w_4P(True|Sunny, Cool, High,No)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='#E8800A'>Enter Naive Bayes</font>\n",
    "\n",
    "Naive Bayes simplifies the Bayesian logic by making very stupid, but mathematically convenient, assumptions:\n",
    "1. All features are independent\n",
    "2. All features are equally important\n",
    "3. The probability distribution of the data follows a distribution that we know how to model (e.g. a Gaussian distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these assumptions, we can simplify the above equations to:\n",
    "\n",
    "$P(Sunny, Cool, High, True|Yes) = w_1P(Sunny|Yes) * w_2P(Cool|Yes) * w_3P(High|Yes) * w_4P(True|Yes)$\n",
    "\n",
    "and\n",
    "\n",
    "$P(Sunny, Cool, High, True|No) = w_1P(Sunny|No) * w_2P(Cool|No) * w_3P(High|No) * w_4P(True|No)$\n",
    "\n",
    "where $w_1, w_2, w_3, w_4$ are constants that we can ignore when comparing the two equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we are going to create a Naive Bayes classifier from scratch and apply it to the weather dataset. For each possible outcome, we will need to compute the *prior* and the *likelihood* of each feature given the outcome variable.\n",
    "\n",
    "Then, we will need to check what is the probability of the new instance to belong to each class and select the most likely class, thus making a **prediction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2:`__ Let's start with y = Yes.\n",
    "\n",
    "$P(Yes|today) = \\frac{P(Sunny|Yes) * P(Cool|Yes) * P(High|Yes) * P(True|Yes) * P(Yes)}{P(today)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2.1:`__ Calculate each part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p1 = P(Outlook = Sunny|Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p2 = P(Temperature = Cool|Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p3 = P(Humidity = High|Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p4 = P(Wind = True|Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5 = P(Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p_yes_today = P(Yes|today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3:`__ Now, we need to do the same for y = No.\n",
    "\n",
    "$P(No|today) = \\frac{P(Sunny|No) * P(Cool|No) * P(High|No) * P(True|No) * P(No)}{P(today)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3.1:`__ Calculate each part in the same manner as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4:`__ Normalize the results to obtain something similar to a probability (The sum of the probabilities for *Yes* and *No* should be 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Result:__ The outcome of today is No!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can use the sklearn Naive Bayes for categorical features to confirm our result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1:`__ Import CategoricalNB from sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2:`__ Assign to the object `data` the dataset excepting the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = weather.drop(['PLAY'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3:`__ Assign to the object `target`the dependent variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame(weather['PLAY'], columns = ['PLAY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4:`__ Encode the dataset to apply the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.1:`__ Import the OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.2:`__ Create two instances of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1 = OrdinalEncoder() #encoder for features\n",
    "enc2 = OrdinalEncoder() #encoder for labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.3:`__ Fit the encoder to the data and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc1.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(enc1.transform(data), columns = data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = enc2.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5:`__ Split the dataset into X_train, X_val, y_train and y_val, defining `test_size` as 0.25 , `random_state` equal to 5 and `stratify` by the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = #CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 6:`__ Using CategoricalNB, create a Naive Bayes classifier instance called modelNB2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNB = CategoricalNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods in CategoricalNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 7:`__ Use the `.fit()`method of model to fit the model to the array of points `X_train` and `y_train`,i.e., associate the argument keyword `X` to `X_train` and `y` to `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 8:`__ Use the `.predict()` method to perform classification in `X_train` and assign to the object `labels_train`. Do the same for `X_val` and assign to the object `labels_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = #CODE HERE\n",
    "labels_val = #CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 9:`__ Use the `.predict_proba()` method to obtain the probability estimates for the `X_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 10:`__ Use the `.score()` method of modelNB to obtain the mean accuracy of the given train data `X_train` and the true labels for X, `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 11:`__ Use the `.score()` method of modelNB to obtain the mean accuracy of the given test data `X_val` and the true labels for X, `y_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 12:`__ Make prediction for today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = (['Sunny', 'Cool', 'High', 'True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 13:`__ Transform the data with the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = enc1.transform([today])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 13.1:`__ Make the prediction for the encoded datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = #CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 13.2:`__ Reverse the encoding to understand the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc2.inverse_transform([result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"company\">\n",
    "\n",
    "## Tugas Dataset\n",
    "</a>\n",
    "\n",
    "Tugas is a online Portuguese retailer offering an assortment of goods within 5 major categories: \n",
    "\n",
    "1. Clothes\n",
    "2. Housekeeping\n",
    "3. Kitchen\n",
    "4. Small appliances\n",
    "5. Toys\n",
    "\n",
    "Tugas started a loyalty program 2 years ago. Among other objectives, the program’s aim is to gather Customer information to better drive the marketing efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the description of the data. Note that `Education`and `Marital_Status`are categorical variables that have been transformed into dummys.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image\\variables.png\" style=\"height:450px\">\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1:`__ Import Tugas Dataset (don't forget to import pandas to read the excel file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tugas = pd.read_excel('tugas_dataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2:`__ It is time to explore and understand the data we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2.1:`__ Check the first five rows of the dataset `tugas_dataset` using the method `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2.2:`__ Using the method `.info()`, check the data types of the variables of `tugas_dataset` and if there are any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tugas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2.3:`__ Get the main descriptive statistics for all the variables in `tugas_dataset` using the method `.describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tugas.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3:`__ After the exploration and understanding of data, we need to fix possible problems on data like missing values or outliers and we can create new variables in order to get variables with higher predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Incoherences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3.1:`__ After exploring the dataset, in this step check for any kind of mismatches, inconsistencies, __incoherence__ and redundancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tugas['Recomendation'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the description of the dataset, it is written that the variable _Recomendation_ only assumes values from 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tugas.loc[(df_tugas['Mnt'] > 0) & (df_tugas['Frq'] == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are any clients that have spent money, but do not have any purchase registered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3.2:`__ The __outliers__ can be a result of a mistake during data collection or it can be just an indication of variance in your data. Check if there are any since these can negatively influence the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hk_mean = df_tugas['HouseKeeping'].mean()\n",
    "hk_std = df_tugas['HouseKeeping'].std()\n",
    "\n",
    "df_tugas.loc[df_tugas['HouseKeeping'] > hk_mean + 5 * hk_std]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3.3:`__ Handling __missing data__ is important as many machine learning algorithms do not support data with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tugas['Income'] = df_tugas['Income'].fillna(df_tugas['Income'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tugas.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4:`__ It is time to create a model. At this step, we are going to implement the Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Gaussian Naive Bayes</font> <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "  [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start with a dataset called tugas, which has continuous and categorical attributes and a binary target variable. \n",
    "Because we have mostly continuous attributes we will be using Gaussian Naive Bayes, so the likelihood of the features is assumed to be Gaussian:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "P(x_{i}\\mid y) = \\frac{1}{\\sqrt{2\\pi \\sigma_y^{2}}} \\exp \\left(-\\frac{(x_{i} -\\mu_{y})^2}{2\\sigma_y^{2}} \\right)\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.1:`__ Import GaussianNB from sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.2:`__ Assign to the object `data` the values from tugas excepting the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_tugas.drop(['DepVar', 'Custid'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.3:`__ Assign to the object `target`the dependent variable from tugas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_tugas['DepVar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.4:`__ Split the dataset tugas into X_train, X_val, y_train and y_val, defining `test_size` as 0.25 , `random_state`equal to 5 and `stratify` by the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.25, stratify = target, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.5:`__ Irrelevant or partially relevant features can negatively impact model performance. Besides that, having correlated independent data can also harm the model. In this way, check the correlation matrix in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def cor_heatmap(cor):\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(data = cor, annot = True, cmap = plt.cm.Reds, fmt='.1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_pearson = X_train.corr()\n",
    "cor_heatmap(cor_pearson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide which variables to keep or withdraw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['Income', 'Year_Birth'], axis=1)\n",
    "\n",
    "#do the same for validation\n",
    "X_val = X_val.drop(['Income', 'Year_Birth'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.6:`__ Using GaussianNB, create a Naive Bayes classifier instance called modelNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNB = GaussianNB(var_smoothing=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods in GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.7:`__ Use the `.fit()`method of model to fit the model to the array of points `X_train` and `y_train`,i.e., associate the argument keyword `X` to `X_train` and `y` to `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNB.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.8:`__ Use the `.predict()` method of modelNB to perform classification in `X_train` and assign to the object `labels_train`. Do the same for `X_val` and assign to the object `labels_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = modelNB.predict(X_train)\n",
    "labels_val = modelNB.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.9:`__ Use the `.predict_proba()` method of modelNB to obtain the probability estimates for the `X_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.10:`__ Use the `.score()` method of modelNB to obtain the mean accuracy of the given train data `X_train` and the true labels for X, `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.11:`__ Use the `.score()` method of modelNB to obtain the mean accuracy of the given test data `X_val` and the true labels for X, `y_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes in GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.12:`__ Call the attribute `.class_prior_` in modelNB to check the probability of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNB.class_prior_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here you can verify that the probability of the dependent variable being one in our dataset is only 7% from the total dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.13:`__ Call the attribute `.class_count_` in modelNB to verify the number of training samples observed in eacg class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNB.class_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training dataset, 1744 observations have 0 as dependent variable, while only 131 observations have 1 as dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.14:`__ Call the attribute `.theta_` in modelNB to verify the mean of each feature per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNB.theta_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some variables we can verify that the mean is distinct taking into account the dependent variable associated. Take as example the variable Mnt (in the seventh column):\n",
    "- For the customers where the dependent variable is 0, the value of Mnt is around 573 monetary units;\n",
    "- For the customers who bougth the product (dependent variable = 1), the money spent is around 1746 monetary units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ If you dont remember the order of the variables, you can always call `data.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4.15:`__ Call the attribute `.var_` in modelNB to verify the variance of each feature per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelNB.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the attribute var, we obtain the variance of each variable, for both labels (0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5:`__ We already have the ground truth and the predicted values. In this way we can start evaluating the performance of our model in the train and the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presenting the classification report and the confusion matrix for Train and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5.1:`__ To evaluate the results, we are going to use the classification report method that will return the main classification metrics. <br>\n",
    "Import `classification_report` and `confusion_matrix` from sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5.2:`__: Create  a function named `metrics` that will print the results of the classification report and the confusion matrix for both datasets (train and validation) _(written for you)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_train, pred_train , y_val, pred_val):\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                     TRAIN                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_train, pred_train))\n",
    "    print(confusion_matrix(y_train, pred_train))\n",
    "\n",
    "\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                VALIDATION                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_val, pred_val))\n",
    "    print(confusion_matrix(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5.3:`__ Call the function metrics() defined previously, and define the arguments: <br> (`y_train = y_train`, `pred_train = labels_train` , `y_val = y_val`, `pred_val = labels_val`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(y_train, labels_train, y_val, labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix in sklearn is presented in the following format: <br>\n",
    "[ [ TN  FP  ] <br>\n",
    "    [ FN  TP ] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have the main results for your training and validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#E8800A'>Exercise</font><a class=\"anchor\" id=\"third-bullet\"></a>\n",
    " [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Exercise 1:`__ Create a function for the tennis dataset that given the features of an instance can predict the outcome! <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(outlook, temperature, humidity, windy):\n",
    "    \n",
    "    #calculate the probability of playing today\n",
    "    a1 = len(weather[(weather.PLAY == 'Yes') & (weather.OUTLOOK == outlook)]) / len(weather[weather.PLAY == 'Yes'])\n",
    "    a2 = len(weather[(weather.PLAY == 'Yes') & (weather.TEMPERATURE == temperature)]) / len(weather[weather.PLAY == 'Yes'])\n",
    "    a3 = len(weather[(weather.PLAY == 'Yes') & (weather.HUMIDITY == humidity)]) / len(weather[weather.PLAY == 'Yes'])\n",
    "    a4 = len(weather[(weather.PLAY == 'Yes') & (weather.WINDY == windy)]) / len(weather[weather.PLAY == 'Yes'])\n",
    "    a5 = len(weather[weather.PLAY == 'Yes'])/len(weather)\n",
    "    p_yes_today = a1 * a2 * a3 * a4 * a5\n",
    "    \n",
    "    #repeat the same for no \n",
    "    b1 = len(weather[(weather.PLAY == 'No') & (weather.OUTLOOK == outlook)]) / len(weather[weather.PLAY == 'No'])\n",
    "    b2 = len(weather[(weather.PLAY == 'No') & (weather.TEMPERATURE == temperature)]) / len(weather[weather.PLAY == 'No'])\n",
    "    b3 = len(weather[(weather.PLAY == 'No') & (weather.HUMIDITY == humidity)]) / len(weather[weather.PLAY == 'No'])\n",
    "    b4 = len(weather[(weather.PLAY == 'No') & (weather.WINDY == windy)]) / len(weather[weather.PLAY == 'No'])\n",
    "    b5 = len(weather[weather.PLAY == 'No'])/len(weather)\n",
    "    p_no_today = b1 * b2 * b3 * b4 * b5\n",
    "    \n",
    "    #normalize results to ensure probability of event and complement = 1\n",
    "    P_yes_today = p_yes_today / (p_yes_today + p_no_today)\n",
    "    P_no_today = p_no_today / (p_yes_today + p_no_today)\n",
    "    \n",
    "    #make decision\n",
    "    if P_yes_today > P_no_today:\n",
    "        outcome = 'Yes'\n",
    "    else:\n",
    "        outcome = 'No'\n",
    "    return outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result('Sunny', 'Cool', 'High', 'True')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
